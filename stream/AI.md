# Stream of thoughts on AI

- 19 Jan 2023

Getty Images sues Stable Diffusion.... in the UK.
With their IP friendly courts and innovative precedents such as Temple Island Collections Ltd v New English Teas Ltd (that extends copyrights to photograph concepts in addition to their expressions), I'm pretty sure the claimants will breeze through the courts with a solid win (just like Amber).

- 29 Jan 2023

ChatGPT å€‹ G ä¿‚ ã€ŒGenerativeã€ï¼Œå…¶å¯¦æœ¬èº«ä¿‚ä¸€å€‹ã€Œæ–‡å­—ç”Ÿæˆå™¨ã€ã€‚ä½œç‚ºä¸€å€‹ã€Œèªè¨€æ¨¡å‹ã€(language model)ï¼Œä½¢ä¸»è¦è¨“ç·´å˜…ä¿‚ã€Œèªè¨€èƒ½åŠ›ã€ï¼Œå””ä¿‚èªè­˜ç¾å¯¦ä¸–ç•Œå˜…ä»»ä½•å˜¢ã€‚èˆ‡å…¶ç•¶ä½¢ä¿‚ä¸€å€‹ã€Œæœƒä¿¾éŒ¯ç­”æ¡ˆå˜…Googleã€ï¼Œæˆ‘è¦ºå¾—æ‡‰è©²ç•¶ä½¢ä¿‚ä¸€å€‹é³©å™å¤§å¸«ï¼ˆè¤’ç¾©ï¼‰ã€‚å¥½è½å°‘å°‘å«ã€Œæ–‡å¦“ã€ï¼Œæ­£å¼å•²å¯ä»¥è©±ä¿‚ã€Œæ–‡è†½ã€ã€‚å³ä¿‚ä½ å«ä½¢å¯«å•²å˜¢ï¼Œä½¢å¯ä»¥æ–‡é€šå­—é †ã€è¡Œæ–‡æµæš¢ï¼Œå°±ç®—é¡›å€’é»‘ç™½éƒ½å†‡å•é¡Œï¼›ä½†å…§å®¹ä¿‚å’ªæº–ç¢ºï¼Œå°±é–£ä¸‹è‡ªç†ã€‚æ‰€ä»¥æœ€åˆé©å˜…ç”¨æ³•ï¼Œå°±ä¿‚æœ¬èº«å¾—å…©å¥å˜…å…§å®¹ï¼Œä½ æƒ³å˜”äº”ç™¾å­—å¤§æ–¹å¾—é«”å˜…èªªè©±å‡ºåšŸå—°é™£ç”¨ä½¢ã€‚å¦‚æœä¸Šé¢å˜…è¬›æ³•éƒ½å¤ªæŠ½è±¡ï¼Œæˆ–è€…å’è¬›å¤§å®¶æœƒæ˜ï¼š

â‹¯ã€€ä½ ç•¶ä½¢ä¿‚19æ‰å­å°±å•±ã—å–‡ã€‚

- 11 Feb 2023

GPT can be implemented in less than 100 lines of code. I'm not sure anyone actually understands the implications of this.

We have through experimental science proven that, there exist relatively simple systems where if you feed them sufficient amounts of data, they will become as intelligent the data allows. The threshold is at most some tens of billions of repeating "units", possibly less, organized in a way that follows a gradient and settles on local optimum state (which many physical processes do naturally). This is obviously less complicated than the simplest lifeforms we know on Earth!!

So, we have basically proven that life is not a prerequisite for intelligence, and that intelligence can be structurally less complex than life. Think about all the possibilities. Elementals, Nature Spirits, Gaia, God...

- 16 Feb 2023

The extent that language models can write code but can't do math is evidence that programming (of the kind language models do well) is a language skill, not a "math" or "logic" skill (to the extent that math or logic is not a language skill).

I have long argued that software engineers should take courses in creative writing instead of linear algebra and calculus (if one must choose). We haven't proven this is better yet, but at least we've seen (IIRC, read it somewhere but I don't have references, appreciate if anyone can suggest some) that the natural language ability of language models are improved after being trained on computer language corpora....

Note: the qualifications/reservations in the brackets are important.

- 18 Feb 2023

This might have been obvious in retrospect - if we adopt the Turing Test definition of intelligence, a language model that accurately "predicts the next word" of human text is obviously going to be intelligent.

- 20 Feb 2023

è¦‹è¦ªå•²å€‹ CV æœ‰ã€ŒEngineeringã€å‘¢å€‹å­—å˜…äººè¬› ChatGPT ä¹œä¹œæŸ’æŸ’å°±è¦ºå¾—å¥½ç¬‘ã€‚ä½ ä¼°çœŸä¿‚æœ‰å€‹é›»å­—å°±ä»£è¡¨ä½ è­˜ï¼Ÿ

- 27 Feb 2023

In terms of entropy, Intelligence ~= Compression ~= Encryption ~= Random Noise

The difference between these four things is literally a matter of subjective interpretation.

Now, look at nature, look at all those sources of random noise. They say it's just random noise, nothing intelligent is going on. It's as if you look into a TLS session and conclude that humanity is just transmitting zetabytes of white noise every day ğŸ˜ƒ

PS: what does the so called "laws of thermodynamics" really imply with the entropy increase then? In general I think the prevalent interpretation by physicists of entropy is a bit too narrow-minded. Randomness is merely a measure of our lack of knowledge or capacity to understand.

- 15 Mar 2023

GPT-4 åƒåŠ è€ƒè©¦ã€‚åŸºæœ¬ä¸Šç§‘ç§‘æˆç¸¾éƒ½ä¿‚ 80+% percentileï¼ŒåŒ…æ‹¬è€ƒå¾‹å¸«è©¦æœ‰90%ï¼Œ æ¯” GPT-3.5 é€²æ­¥å’—å””å°‘ ï¼ˆè­‰æ˜è®€å¤šå•²æ›¸çœŸä¿‚å¥½ç·Šè¦ [èª¤]ï¼‰

å¥½åœ¨æˆ‘ç© codeforces åŒ leetcode å¯èƒ½ä»²æœ‰æ©Ÿæœƒè´ä½¢â‹¯

å»åˆ°åŸç‰ˆ ChatGPT/GPT3.5 å˜…å¹´ä»£ï¼Œæˆ‘å·²ç¶“æ„Ÿè¦ºåˆ°ä½¢ gen å‡ºåšŸå˜…å˜¢æ¯”ä¸€èˆ¬æ™®é€šäººå˜…æ°´å¹³ç‚ºé«˜ã€‚åŠé–“ä¸æ–·è©± AI æœ‰å‘¢æ¨£å—°æ¨£é™åˆ¶ï¼Œæ ¹æœ¬ä¿‚å•²äºº hallucinate å‡ºåšŸå˜…è«–è¿°ï¼Œæ½›æ„è­˜ä»¤è‡ªå·±å¥½éå•²ã€‚GPT åŸºç¤æŠ€è¡“ä¾†è‡ª 2017 å¹´ä¸€ä»½è«–æ–‡ ("Attention Is All You Need" [åˆ©ç”³å†‡ç‡é])ï¼Œå¤§å®¶å°±æ”ä½ä¸€å€‹é–‹ç™¼å’—äº”å¹´æœªå®Œå…¨æˆç†Ÿå˜…æ–°æŠ€è¡“é³©å™ä½¢æœªä¾†å¹¾åå¹´æœƒæœ‰å’©é™åˆ¶ï¼Œå””å»è«—ä¸‹ä½¢æœƒè¶ŠåšŸè¶Šå‹ï¼Œå‘¢ç¨®å¿ƒæ…‹å…¶å¯¦çœŸä¿‚å¹¾å¥½ç¬‘ä¸‹ã€‚

ã€Œå“ï¼Œèª²æº«ï¼Œçª©è¥Ÿæ‹—æ±ªæ³¢æ­åŒ–å›‰å¤šsã€

https://openai.com/research/gpt-4

- 16 Mar 2023

ç‡å•²äººè¨è«–ã€Œå¾Œ AI å¹´ä»£æ‡‰è©²æ•™å€‹ä»”å’©æŠ€èƒ½ã€è¦ºå¾—äººé¡çœŸä¿‚å¹¾å¯æ„›ã€‚

æœ¬èº«æˆ‘å“‹ä¸Šä¸€ä»£ (ä½ ç•¶æˆ‘ä¿‚è¬›ç·Š boomer generationå•¦) å·²ç¶“å””å¤šè­˜æ•™æˆ‘å“‹å‘¢ä»£äººè¦å­¸å’©æŠ€èƒ½ï¼Œä»Šæ—¥å˜… 80/90 å¾Œåˆé»å¯èƒ½è¦ºå¾—æœƒã€Œç ”ç©¶ã€åˆ°ä¸€å€‹æ­£ç¢ºå˜…æ–¹å‘ï¼Œå””æœƒè€é»ä»”å¥³å‘¢ï¼Ÿ

å€‹ inconvenient truth ä¿‚ï¼ŒAI äº¦å·²ç¶“å–ä»£åŸ‹ã€Œå¹«ä»”å¥³èˆ–è·¯åšæˆåŠŸäººå£«ã€å‘¢ç¨®ã€Œå·¥ä½œã€ã€‚è¦ºå¾—è‡ªå·±å»éä»”å¥³å˜…åŒæ™‚ï¼Œåˆæƒ³å€‹ä»”å¥³æˆåŠŸéè‡ªå·±ï¼Œæœ¬èº«ä¿‚å¹¾çŸ›ç›¾å˜…è«—æ³•åšŸã€‚

å¤§éƒ¨ä»½äººä¿‚å†‡ä¹œã€Œå…ˆçŸ¥åŠ›ã€å˜…ï¼Œä¸–ç•Œå˜…å˜¢ä½ ä¼°å””æ’šåˆ°ã—å–‡ã€‚

- 17 Mar 2023

Language models are going to be pocket sized. LLaMA and its derivatives already fit inside a phone (currently runs very hot and slow but things are just a couple optimizations away).

Super-human AGI models are going to be the new super computers. Imagine a beowulf cluster of these solving the hardest problems that have stumped humans for decades (P=NP?) An implied intuition of the Turing Test is that general language ability ~= intelligence. I can't imagine these efforts not working out at least to some extent given the pace of progress.

- 17 Mar 2023

A couple (10+) years ago the book "Thinking, Fast and Slow" by Daniel Kahneman popularized the idea of two modes of thinking: "System 1" and "System 2". "System 1" is fast, instinctive and emotional; "System 2" is slower, more deliberative, and more logical.

GPT and other language models are basically similar to "System 1".  The models don't have a way to pause to think (any pauses you encounter are due to high load from other users...) .  I find it actually quite amazing that the AI models can produce high quality results without "pausing to think", most experts can't do that without some deliberation and planning before they give an answer to a complex question.

When we pause to think, we often brainstorm some likely answers and evaluate the merits of each of them, kind of like having a mini-debate inside one's head.  I wonder whether this technique can work on GPT models too -- i.e. ask it to debate itself critically for a couple thousands of words, and then come up with a final answer. It would also be interesting to allow it to request online resources to look up factual data, before answering questions. It's also possible that you could ask it to use different "personalities" to look at its answer from different "perspectives", so that mistakes are further minimized.

I imagine that as long as the token limit is raised, it would be trivial to prompt the AI model to incorporate all the techniques above and loop through them a couple times to refine an answer. That answer is likely to result in super-human reasoning abilities (at least in many topics/fields/subjects), I think.

That is why I think those betting against AI capabilities is going to lose hard. If even an amateur like me can think of ways to trivially improve its performance, one can't help but wonder how many crazy things the bleeding edge researchers are doing...

The only question is whether they'd even let us know. Good thing about USA is that people like to blow whistles, so... chances are still good.

- 23 Mar 2023

Re: https://arxiv.org/abs/2303.12712

This is why the Ivory Tower and associated industries are rotten to the core.

Only 9 months ago Blake Lemoine was thoroughly ridiculed, with so called experts going so far as to claim that he was suffering from the â€œELIZAâ€ effect. The level of certainty they made the claim was mind boggling, only surpassed by the US health officials' claim that wearing masks donâ€™t help prevent covid.

Now, when respected researchers who couldnâ€™t tell you what a â€œsoulâ€ is claims the same thing, they can pretend theyâ€™re the first to do so.

